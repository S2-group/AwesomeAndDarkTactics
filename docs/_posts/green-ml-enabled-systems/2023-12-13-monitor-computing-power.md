---
layout: tactic

title:  "Monitor Computing Power"
tags: machine-learning management
t-sort: "Awesome Tactic"
t-type: "Architectural Tactic"
categories: green-ml-enabled-systems
t-description: "Estimating and calculating the energy footprint of a machine learning model can help to reduce the computational power of ML models. Monitoring the energy consumption of a ML model in the long term helps to identify those components where energy is being inefficiently utilized. This can serve as a starting point for making improvements to reduce energy consumption. There has been a lack of easy-to-use tools to do that, but recently  researchers have provided frameworks for how to estimate or calculate the energy footprint of machine learning."
t-participant: "Data Scientist"
t-artifact: "Machine Learning Model"
t-context: "General"
t-feature: 
t-intent: "Improve energy efficiency by monitoring computing power of machine learning in the long term"
t-targetQA: "Energy Efficiency"
t-relatedQA: 
t-measuredimpact: 
t-source: "Qingqing Cao, Yash Kumar Lal, Harsh Trivedi, Aruna Balasubramanian, and Niranjan Balasubramanian. 2021. IrEne: Interpretable Energy Prediction for Transformers. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (2021). [DOI](https://doi.org/10.48550/arXiv.2106.01199); Mohit Kumar, Xingzhou Zhang, Liangkai Liu, Yifan Wang, and Weisong Shi. 2020. Energy-Efficient Machine Learning on the Edges. In 2020 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW). 912â€“921. [DOI](https://doi.org/10.1109/IPDPSW50202.2020.00153)"
t-source-doi: 
t-diagram: "monitor-computing-power.png"
---