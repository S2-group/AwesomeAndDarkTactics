---
layout: tactic

title:  "Use Quantization-Aware Training"
tags: machine-learning model-training
t-sort: "Awesome Tactic"
t-type: "Architectural Tactic"
categories: green-ml-enabled-systems
t-description: "Quantization-aware training is a technique used to train neural networks to convert data types to lower precision. The idea is to use fixed-point or integer representations instead of the more commonly used higher-precision floating-point representations. This improves the performance and energy efficiency of the model in federated learning."
t-participant: "Data Scientist"
t-artifact: "Model"
t-context: "Machine Learning"
t-feature: "Model Training"
t-intent: "Improve energy efficiency by using quantization-aware training to convert high-precision data types to lower precision"
t-targetQA: "Accuracy"
t-relatedQA: "Energy Efficiency"
t-measuredimpact: 
t-source: "Minsu Kim, Walid Saad, Mohammad Mozaffari, and Merouane Debbah. 2021. On the Tradeoff between Energy, Precision, and Accuracy in Federated Quantized Neural Networks. In ICC 2022 - IEEE International Conference on Communications. 2194â€“2199. [DOI](https://doi.org/10.1109/ICC45855.2022.9838362); Martino Sorbaro, Qian Liu, Massimo Bortone, and Sadique Sheik. 2020. Optimizing the Energy Consumption of Spiking Neural Networks for Neuromorphic Applications. Frontiers in Neuroscience 14 (2020), 662. [DOI](https://doi.org/10.3389/fnins.2020.00662)"
t-source-doi: 
t-diagram: "use-quantization-aware-training.png"
---