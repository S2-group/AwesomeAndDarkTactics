---
layout: tactic

title:  "Decrease Model Complexity"
tags: machine-learning algorithm-design
t-sort: "Awesome Tactic"
t-type: "Architectural Tactic"
categories: green-ml-enabled-systems
t-description: "Complex AI models have shown to have high energy consumption and therefore scaling down model complexity can contribute to environmental sustainability. Simplifying the model structure can lead to faster training and inference times, making it more efficient to deploy and use in real-world applications. For example, using simple three-layered Convolutional Neural Network (CNN) architectures (Morotti et al, 2021) and shallower Decision Trees (Abreu et al, 2020) has shown to be energy-efficient while still providing high levels of precision."
t-participant: "Data Scientist"
t-artifact: "Algorithm"
t-context: "Machine Learning"
t-feature: "Inference"
t-intent: "Improve energy efficiency by decreasing model complexity while still meeting accuracy requirements."
t-targetQA: "Energy Efficiency"
t-relatedQA: 
t-measuredimpact: 
t-source: "Brunno A Abreu, Mateus Grellert, and Sergio Bampi. 2020. VLSI Design of Tree-Based Inference for Low-Power Learning Applications. In 2020 IEEE International Symposium on Circuits and Systems (ISCAS). IEEE, 1â€“5. [DOI](https://doi.org/10.3390/jimaging7080139); Elena Morotti, Davide Evangelista, and Elena Loli Piccolomini. 2021. A Green Prospective for Learned Post-Processing in Sparse-View Tomographic Reconstruction. Journal of Imaging 7, 8 (2021), 139. [DOI](https://doi.org/10.1109/ISCAS45731.2020.9180704)"
t-source-doi: 
t-diagram: "decrease-model-complexity.png"
---